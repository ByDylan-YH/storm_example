Strom的具体应用场景
1：Storm实时计算一些中间数据，把数据存储到Redis这种内存数据库中，供其他人调用
2：Storm对一些数据进行实时统计，把统计的结果存储到一些类似于mysql的关系型数据库中，最终可以出图表



项目实现；

注意：针对Flume实时日志数据采集，在这里我们使用kakfa的生产者来模拟

zookeeper：hadoop100
kafka：hadoop100
storm: hadoop100 hadoop101 hadoop102

mysql: localhost(windows机器)

1：先把zk、kafka、storm等服务启动
2：在kafka中创建topic
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 5 --topic access_log
3：实现kafka的生产者，模拟向access_log这个topic产生数据【验证topic以及生产者代码是否可用】
	bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic access_log --from-beginning
4：开发topology代码
	组装kafkaSpout
	开发自定义bolt
5：在本地执行代码
6：把代码打jar包提交到集群上运行
7：运行webui程序，进行展现

需要注意的问题
1：kafkaSpout默认开启了acker消息确认机制，需要在后面的bolt中调用ack进行确认
2.免费代理IP
https://www.xicidaili.com/
3.根据IP获取城市
https://freeapi.ipip.net/113.45.32.253